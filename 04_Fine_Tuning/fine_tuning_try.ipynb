{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fschulz/miniconda3/envs/python3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    ")  # Gives easier dataset managment and creates mini batches\n",
    "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2755538\n",
      "2755538\n"
     ]
    }
   ],
   "source": [
    "# Network architecture - DON'T CHANGE\n",
    "# -*- coding:utf-8 -*-\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "cfg = {\n",
    "    12: [1, 1, 1, 1, 1],\n",
    "    18: [1, 2, 2, 2, 1],\n",
    "    20: [1, 2, 4, 1, 1],\n",
    "    28: [1, 3, 6, 1, 1],\n",
    "    36: [2, 4, 8, 2, 1],\n",
    "    64: [3, 8, 16, 3, 1],\n",
    "}\n",
    "\n",
    "block2channels = {\n",
    "    0: 16,\n",
    "    1: 32,\n",
    "    2: 64,\n",
    "    3: 128,\n",
    "    4: 256\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n",
    "                     padding=1)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv2d_1 = conv3x3(in_channels, out_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2d_2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv2d_3 = conv3x3(out_channels, out_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        net = self.conv2d_1(x)\n",
    "        net = self.bn1(net)\n",
    "        net = self.relu1(net)\n",
    "\n",
    "        net = self.conv2d_2(net)\n",
    "        net = self.bn2(net)\n",
    "        net = self.relu2(net)\n",
    "\n",
    "        net = self.conv2d_3(net)\n",
    "        net = self.bn3(net)\n",
    "        net = self.relu3(net)\n",
    "\n",
    "        if x.size(1) < net.size(1):\n",
    "            x = F.pad(x, x.view(1) - net.view(1))\n",
    "\n",
    "        # if the num of channels of the input is larger than the outputs' - don't use the residual connection\n",
    "        if x.size(1) > net.size(1):\n",
    "            pass\n",
    "        else:\n",
    "            net = net + x\n",
    "        return net\n",
    "\n",
    "\n",
    "class DownSampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownSampleBlock, self).__init__()\n",
    "        self.conv2d = conv3x3(in_channels, out_channels, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(self.relu(self.conv2d(x)))\n",
    "\n",
    "\n",
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UpSampleBlock, self).__init__()\n",
    "        self.us = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.us(x)\n",
    "\n",
    "\n",
    "class BackboneBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, repetitions, keep_res=False):\n",
    "        super(BackboneBlock, self).__init__()\n",
    "        self.keep_res = keep_res\n",
    "        if keep_res is False:\n",
    "            self.down_sample_block = DownSampleBlock(in_channels, out_channels)\n",
    "            self.res_blocks = nn.ModuleList([ResBlock(out_channels, out_channels)] * repetitions)\n",
    "        else:\n",
    "            self.res_blocks = nn.ModuleList([conv3x3(in_channels, out_channels)] +\n",
    "                                            [ResBlock(out_channels, out_channels)] * repetitions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.keep_res is False:\n",
    "            net = self.down_sample_block(x)\n",
    "        else:\n",
    "            net = x\n",
    "        for res_block in self.res_blocks:\n",
    "            net = res_block(net)\n",
    "        return net\n",
    "\n",
    "\n",
    "class HE_Classifier(nn.Module):\n",
    "    def __init__(self, input_size=512, input_channels=3, sphereface_size=12, net_dropout_prob=0.1):\n",
    "        super(HE_Classifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        res_blocks = cfg[sphereface_size]\n",
    "\n",
    "        self.block1 = BackboneBlock(input_channels, block2channels[0], res_blocks[0], keep_res=True)\n",
    "        self.block2 = BackboneBlock(block2channels[0], block2channels[1], res_blocks[1])\n",
    "        self.block3 = BackboneBlock(block2channels[1], block2channels[2], res_blocks[2])\n",
    "        self.block4 = BackboneBlock(block2channels[2], block2channels[3], res_blocks[3])\n",
    "        self.block5 = BackboneBlock(block2channels[3], block2channels[4], res_blocks[4])\n",
    "        self.sphereface_blocks = nn.ModuleList([self.block1, self.block2, self.block3, self.block4, self.block5])\n",
    "\n",
    "        f_size = input_size // (2 ** 4)\n",
    "        self._gap = nn.AvgPool2d((f_size, f_size), stride=1)\n",
    "        self._final_1x1_conv = nn.Conv2d(in_channels=block2channels[4], out_channels=2, kernel_size=1)\n",
    "        self.net_dropout = torch.nn.Dropout(p=net_dropout_prob)\n",
    "\n",
    "    def get_num_trainable_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def get_im_size(self):\n",
    "        return self.input_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        for block in self.sphereface_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # predict class\n",
    "        x = self.net_dropout(x)\n",
    "        x = self._gap(x)\n",
    "        x = self._final_1x1_conv(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "net = HE_Classifier(input_size=512, input_channels=3, sphereface_size=12)\n",
    "\n",
    "print(net.get_num_params())\n",
    "print(net.get_num_trainable_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze only last 1x1 conv layer, has 2 trainable parameters\n",
    "# In total, model has 80 parameters\n",
    "count = 0\n",
    "for param in net.parameters():\n",
    "    if count < 78:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n",
      "Done\n",
      "10\n",
      "422\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training, loading the data\n",
    "\n",
    "input_size = 512\n",
    "data_dir = \"./Data\"\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=1, shuffle=False, num_workers=4, drop_last=False) for x in ['train', 'val']}\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "print(len(image_datasets[\"train\"]))\n",
    "print(len(image_datasets[\"val\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "dict_keys(['epoch', 'model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "# Load model weights as checkpoint\n",
    "\n",
    "print('Loading model...')\n",
    "\n",
    "checkpoint = torch.load(\"./trained_model.pt\", map_location=torch.device('cpu'))\n",
    "net.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "print(checkpoint.keys())\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal loss (from https://pytorch.org/vision/stable/_modules/torchvision/ops/focal_loss.html)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.utils import _log_api_usage_once\n",
    "\n",
    "\n",
    "def sigmoid_focal_loss(\n",
    "    inputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    alpha: float = 0.25,\n",
    "    gamma: float = 2,\n",
    "    reduction: str = \"none\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "\n",
    "    Args:\n",
    "        inputs (Tensor): A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets (Tensor): A float tensor with the same shape as inputs. Stores the binary\n",
    "                classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha (float): Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples or -1 for ignore. Default: ``0.25``.\n",
    "        gamma (float): Exponent of the modulating factor (1 - p_t) to\n",
    "                balance easy vs hard examples. Default: ``2``.\n",
    "        reduction (string): ``'none'`` | ``'mean'`` | ``'sum'``\n",
    "                ``'none'``: No reduction will be applied to the output.\n",
    "                ``'mean'``: The output will be averaged.\n",
    "                ``'sum'``: The output will be summed. Default: ``'none'``.\n",
    "    Returns:\n",
    "        Loss tensor with the reduction option applied.\n",
    "    \"\"\"\n",
    "    # Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\n",
    "\n",
    "\n",
    "    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
    "        #print(\"PANIC\")\n",
    "        _log_api_usage_once(sigmoid_focal_loss)\n",
    "    p = torch.sigmoid(inputs)\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = p * targets + (1 - p) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    # Check reduction option and return loss accordingly\n",
    "    if reduction == \"none\":\n",
    "        pass\n",
    "    elif reduction == \"mean\":\n",
    "        loss = loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid Value for arg 'reduction': '{reduction} \\n Supported reduction modes: 'none', 'mean', 'sum'\"\n",
    "        )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising variables for training\n",
    "optimizer1 = optim.RAdam(net.parameters(), lr=0.001) # this is the optimizer for the first 80 epochs\n",
    "optimizer2 = optim.RAdam(net.parameters(), lr=0.0001) # this is the optimizer for the last 30 epochs\n",
    "batch_size = 32\n",
    "num_epochs = 111\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, dataloaders, optimizer1, optimizer2, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "            \n",
    "        if epoch < 80:\n",
    "            optimizer = optimizer1\n",
    "        else:\n",
    "            optimizer = optimizer2\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = sigmoid_focal_loss(outputs, labels)\n",
    "                        loss2 = sigmoid_focal_loss(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = sigmoid_focal_loss(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/109\n",
      "----------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1])) must be the same as input size (torch.Size([1, 2, 1, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model_ft, hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 53\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, optimizer1, optimizer2, num_epochs, is_inception)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 53\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid_focal_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# backward + optimize only if in training phase\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 42\u001b[0m, in \u001b[0;36msigmoid_focal_loss\u001b[0;34m(inputs, targets, alpha, gamma, reduction)\u001b[0m\n\u001b[1;32m     40\u001b[0m     _log_api_usage_once(sigmoid_focal_loss)\n\u001b[1;32m     41\u001b[0m p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(inputs)\n\u001b[0;32m---> 42\u001b[0m ce_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m p_t \u001b[38;5;241m=\u001b[39m p \u001b[38;5;241m*\u001b[39m targets \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m targets)\n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m ce_loss \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p_t) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m gamma)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.9/lib/python3.9/site-packages/torch/nn/functional.py:3148\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3145\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1])) must be the same as input size (torch.Size([1, 2, 1, 1]))"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "\n",
    "model_ft, hist = train_model(net, dataloaders_dict, optimizer1, optimizer2, num_epochs=num_epochs, is_inception=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python project",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
